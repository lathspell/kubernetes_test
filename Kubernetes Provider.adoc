:toc:

== Digital Ocean (paid)

* https://www.digitalocean.com/products/kubernetes/

== Google Cloud Platform (free tier)

* GCP Free Tier limitations: https://cloud.google.com/free/docs/gcp-free-tier#always-free
* Beware: HTTP Load Balancing is *not* free, use custom NGINX: https://cloud.google.com/compute/pricing?hl=de#lb
* Beware: "f1-micro" nodes only have 600MB RAM with only ca. 100 MB available when using coreos and starting more than one Java app will crash them!

    gcloud beta container --project "kubernetes-2019b-43758" clusters create "free-tier" \
        --region "us-east1" \
        --no-enable-basic-auth \
        --cluster-version "1.12.8-gke.10" \
        --machine-type "f1-micro" \
        --image-type "COS" \
        --disk-type "pd-standard" \
        --disk-size "100" \
        --metadata disable-legacy-endpoints=true \
        --scopes "https://www.googleapis.com/auth/devstorage.read_only","https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/servicecontrol","https://www.googleapis.com/auth/service.management.readonly","https://www.googleapis.com/auth/trace.append" \
        --preemptible \
        --num-nodes "1" \
        --enable-stackdriver-kubernetes \
        --enable-ip-alias \
        --network "projects/kubernetes-2019b-43758/global/networks/default" \
        --subnetwork "projects/kubernetes-2019b-43758/regions/us-east1/subnetworks/default" \
        --default-max-pods-per-node "110" \
        --addons HorizontalPodAutoscaling \
        --enable-autoupgrade \
        --enable-autorepair \
        --maintenance-window "04:00"

    gcloud beta container clusters get-credentials free-tier --region us-east1 --project kubernetes-2019b-43758

Console login to node using:

    gcloud beta compute --project "kubernetes-2019b-43758" ssh --zone "us-east1-d" "gke-free-tier-default-pool-7240688e-wlrp"

=== Cluster Hibernating

To save costs, a cluster can be hibernates by defining that each Zone has 0
instead of 1 Compute Engine Virtual Machine.That can be done by editing the
(usually three) Compute Engine Instance Group definitions (e.g.
gke-free-tier-default-pool-7240688e-65g8) on

    https://console.cloud.google.com/compute/instanceGroups/details/us-east1-d/gke-free-tier-default-pool-7240688e-grp?project=kubernetes-2019b-43758&edit=true&tab=members

== Minikube (local)

=== CLI

Setup

    source <(kubectl completion bash)
    minikube config set memory 4096         # change memory from 2GB to 4GB
    minikube config set cpu 4               # change cpus from 1 to 4
	minikube start
    eval $(minikube docker-env)             # Docker images should be build and stored in the Minikube Docker instance

Status:

	minikube ip		# shows ip like 192.168.99.100
	minikube dashboard

Teardown:

	kubectl delete service hello-node
	kubectl delete deployment hello-node

	minikube stop
	minikube delete

=== Network Access

A Pod (i.e. a Docker container) can listen on as many ports as it wants, only the "exposed" are reachable by other
containers.They are not reachable by other apps on the Host though.For that they have to declare a port forwarding
(docker -p 8080:80).

Kubernetes normally uses more than one host nodes though so the port must not only forwarded to localhost:1234 but to the
external IP of that host node.This is done using a Service, e.g. one with type NodePort.

Applications outside the Kubernetes Cluster still do not see this port as the whole cluster network is fenced off to
the outside. Additionally the outsider would not know which Kubernetes host node currently has an instance of the service
running.A Service of type LoadBalancer is needed here which takes care of forwarding to the port on the right node.
Each service has a unique IP address inside the cluster and LoadBalancer services can additionally use a specific external
IP to bind on.

With Minikube which always only has one node there are several possibilities:

1. Using a NodePort service and forward using a high port (30000-32767) on the Minikube IP.This port range is passed
through 1:1 to the host node.

         $ kubectl get services
         NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                         AGE
         f1b                     NodePort    10.104.40.33     <none>        80:32533/TCP                    16m
         $ http $(minikube ip):32533

2. Use the "minikube tunnel" command.The real port (e.g. 8080) is then available on the IP listed in
"kubectl get service" as "Cluster-IP".Every service has its own IP though!This command also enables
the use of the LoadBalancer service type.Its "External-IP" is the same as the internal one, though.

3. Use "kubectl port-forward service/organizationservice 18080:8080" to forward localhost:18080 to port 8080 of
any Pod in that service.The port-forwarding is not a Kubernetes object but a CLI feature.

4. Use Minikube "ingress" add-on and an Ingres service type.This will put an Nginx or similar proxy, running in
a Pod itself, between the outside world and the internally accessible cluster IPs.Only for HTTP traffic though.

=== Kubectl Proxy

  kubectl proxy                 - Open port 8001 to Kubernetes REST API

    GET localhost:8001/logs/kube-apiserver.log
    GET localhost:8001/healthz/ping
    ...
    GET localhost:8001/api/v1/namespaces/default/pods/hello-server-6f5bdf948c-7ttsv/            - Pod status
    GET localhost:8001/api/v1/namespaces/default/pods/hello-server-6f5bdf948c-7ttsv/proxy/      - HTTP to that Pod without Loadbalancer

=== Docker Images

Kubernetes does not know about any Docker daemons running locally on the desktop.
Minikube can give the local docker CLI access to the Kubernetes docker daemon though.
All images have to be rebuild (or otherwise pushed to this Docker instance).

  eval $(minikube docker-env)
  docker build …
  kubectl run … --image-pull-policy=Never

== Rancher k3s / k3d (local)

Local multi-node installation of a stripped down, yet fully compliant, version of Kubernetes.
The k3d variant runs the master node and all worker nodes as Docker containers.
Worker nodes are called "agents". Can be used with plain kubectl after creation.

    brew install k3d                        - Installation on MacOS

    k3d cluster create ckad --agents=3      - Create cluster with 3 worker nodes
      --k3s-server-arg '--kubelet-arg=eviction-hard=imagefs.available<1%,nodefs.available<1%'
      --k3s-server-arg '--kubelet-arg=eviction-minimum-reclaim=imagefs.available=1%,nodefs.available=1%' 

      # ggf: --k3s-server-arg '--no-deploy=traefik' -p '8081:80@loadbalancer'

    k3d kubeconfig write ckad               - Prints context configuration as YAML
    mv ~/.k3d/kubeconfig-ckad.yaml  ~/.kube/conf.d/ckad.yaml
    ktx k3d-ckad

    k3d image import -c ckad myimage:1.0.0  - Import from local Docker daemon into k3d-ckad cluster
